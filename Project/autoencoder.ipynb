{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions for the Autoencoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import StrMethodFormatter\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to save model weight\n",
    "ae_paths = [\"../models/AE_1c\", \"../models/AE_2c\", \"../models/AE_3c\"]\n",
    "\n",
    "# Batch Size\n",
    "batch_size = 256\n",
    "\n",
    "# Shuffling Parameter for Pipeline\n",
    "shuffle = 1024\n",
    "\n",
    "# Encoder Input Dimension\n",
    "input_shape_img = (64, 64, 7)\n",
    "input_shape_stats = (4,)\n",
    "\n",
    "# Dropout rate for dense layers\n",
    "dropout = 0.5\n",
    "\n",
    "# Encoder Output dimension - Decoder Input Dimension\n",
    "latent_z_dim_labels = 5\n",
    "\n",
    "# Decoder latent dimension\n",
    "latent_dim = (1, 1, 2048)\n",
    "\n",
    "# Number of Training Epochs\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(set=1, test=False):\n",
    "    # Path for the Traning and Validation Data\n",
    "    dataPath = [\n",
    "        \"../data/data_v1.npz\",\n",
    "        \"../data/data_v2.npz\",\n",
    "        \"../data/data_v3.npz\",\n",
    "        \"../data/snr30.npz\",\n",
    "        \"../data/snr60.npz\",\n",
    "    ]\n",
    "\n",
    "    # Load dataset\n",
    "    with np.load(dataPath[set - 1]) as data:\n",
    "        image = data[\"img\"]\n",
    "        image_nonoise = data[\"img_nonoise\"]\n",
    "        label = data[\"label\"]\n",
    "        snr = data[\"snr\"]\n",
    "        sigma = data[\"sigma\"]\n",
    "        psf = data[\"psf_r\"]\n",
    "        psf_img = data[\"psf_img\"]\n",
    "\n",
    "    # Compute Image Statistics (StdDev & Mean of the pixel values of every image)\n",
    "    stats = image.std(axis=(1, 2))\n",
    "    mean = image.mean(axis=(1, 2))\n",
    "\n",
    "    if not test:\n",
    "        # 90% of the data to be kept for training\n",
    "        n_train = int(snr.shape[0] * 0.9)\n",
    "\n",
    "        # Divide PSF Images into training and validation depending on dataset\n",
    "        # Dataset 1 & 2 have fixed PSF and hence only one PSF image\n",
    "        if set == 3:\n",
    "            psf_img_tr, psf_img_val = psf_img[:n_train], psf_img[n_train:]\n",
    "        else:\n",
    "            psf_img_tr, psf_img_val = psf_img, psf_img\n",
    "\n",
    "        # Return Training and Validation Datasets\n",
    "        return dict(\n",
    "            image_train=image[:n_train],\n",
    "            image_val=image[n_train:],\n",
    "            image_nonoise_train=image_nonoise[:n_train],\n",
    "            image_nonoise_val=image_nonoise[n_train:],\n",
    "            label_train=label[:n_train],\n",
    "            label_val=label[n_train:],\n",
    "            psf_train=psf[:n_train],\n",
    "            psf_val=psf[n_train:],\n",
    "            snr_train=snr[:n_train],\n",
    "            snr_val=snr[n_train:],\n",
    "            sigma_train=sigma[:n_train],\n",
    "            sigma_val=sigma[n_train:],\n",
    "            psf_img_train=psf_img_tr,\n",
    "            psf_img_val=psf_img_val,\n",
    "            stats_train=stats[:n_train],\n",
    "            stats_val=stats[n_train:],\n",
    "            mean_train=mean[:n_train],\n",
    "            mean_val=mean[n_train:],\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        return dict(\n",
    "            image_test=image,\n",
    "            image_nonoise_test=image_nonoise,\n",
    "            label_test=label,\n",
    "            psf_test=psf,\n",
    "            snr_test=snr,\n",
    "            sigma_test=sigma,\n",
    "            psf_img_test=psf_img,\n",
    "            stats_test=stats,\n",
    "            mean_test=mean,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data, set=1, test=False):\n",
    "    if not test:\n",
    "        # Number of elements in Training and Validation set\n",
    "        n_train = data[\"sigma_train\"].shape[0]\n",
    "        n_val = data[\"sigma_val\"].shape[0]\n",
    "\n",
    "        # PSF Image handling\n",
    "        if set != 3:\n",
    "            psf_img_tr = np.array(list(data[\"psf_img_train\"]) * n_train)\n",
    "            psf_img_v = np.array(list(data[\"psf_img_train\"]) * n_val)\n",
    "        else:\n",
    "            psf_img_tr = data[\"psf_img_train\"]\n",
    "            psf_img_v = data[\"psf_img_val\"]\n",
    "\n",
    "        # Reshape the Parameter\n",
    "        sigma_t, sigma_v = (data[\"sigma_train\"].reshape(-1, 1), data[\"sigma_val\"].reshape(-1, 1))\n",
    "        psf_t, psf_v = (data[\"psf_train\"].reshape(-1, 1), data[\"psf_val\"].reshape(-1, 1))\n",
    "        stats_t, stats_v = (data[\"stats_train\"].reshape(-1, 1), data[\"stats_val\"].reshape(-1, 1))\n",
    "        mean_t, mean_v = (data[\"mean_train\"].reshape(-1, 1), data[\"mean_val\"].reshape(-1, 1))\n",
    "\n",
    "        # Create Training Dataset\n",
    "        training = tf.data.Dataset.from_tensor_slices(\n",
    "            {\n",
    "                \"Image\": data[\"image_train\"],\n",
    "                \"No-noise Image\": data[\"image_nonoise_train\"],\n",
    "                \"PSF_img\": psf_img_tr,\n",
    "                \"Labels\": data[\"label_train\"],\n",
    "                \"Variance\": data[\"stats_train\"][:, np.newaxis, np.newaxis] ** 2,\n",
    "                \"Sigma\": data[\"sigma_train\"][:, np.newaxis, np.newaxis],\n",
    "                \"Stats\": np.hstack([sigma_t, psf_t, stats_t, mean_t]),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Create Validation Dataset\n",
    "        validation = tf.data.Dataset.from_tensor_slices(\n",
    "            {\n",
    "                \"Image\": data[\"image_val\"],\n",
    "                \"No-noise Image\": data[\"image_nonoise_val\"],\n",
    "                \"PSF_img\": psf_img_v,\n",
    "                \"Labels\": data[\"label_val\"],\n",
    "                \"Variance\": data[\"stats_val\"][:, np.newaxis, np.newaxis] ** 2,\n",
    "                \"Sigma\": data[\"sigma_val\"][:, np.newaxis, np.newaxis],\n",
    "                \"Stats\": np.hstack([sigma_v, psf_v, stats_v, mean_v]),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        return training, validation\n",
    "\n",
    "    else:\n",
    "        n_test = data[\"sigma_test\"].shape[0]\n",
    "        psf_img_te = np.array(list(data[\"psf_img_test\"]) * n_test)\n",
    "        sigma_t = data[\"sigma_test\"].reshape(-1, 1)\n",
    "        psf_t = data[\"psf_test\"].reshape(-1, 1)\n",
    "        stats_t = data[\"stats_test\"].reshape(-1, 1)\n",
    "        mean_t = data[\"mean_test\"].reshape(-1, 1)\n",
    "\n",
    "        # Create testing Dataset\n",
    "        testing = tf.data.Dataset.from_tensor_slices(\n",
    "            {\n",
    "                \"Image\": data[\"image_test\"],\n",
    "                \"No-noise Image\": data[\"image_nonoise_test\"],\n",
    "                \"PSF_img\": psf_img_te,\n",
    "                \"Labels\": data[\"label_test\"],\n",
    "                \"Variance\": data[\"stats_test\"][:, np.newaxis, np.newaxis] ** 2,\n",
    "                \"Sigma\": data[\"sigma_test\"][:, np.newaxis, np.newaxis],\n",
    "                \"Stats\": np.hstack([sigma_t, psf_t, stats_t, mean_t]),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        return testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_noisy_image_to_clean_image(element):\n",
    "    image = element[\"Image\"]\n",
    "    psf = element[\"PSF_img\"]\n",
    "\n",
    "    sigma = element[\"Sigma\"]\n",
    "    var = element[\"Variance\"]\n",
    "    stats = element[\"Stats\"]\n",
    "\n",
    "    # Preprocessing of Images\n",
    "    img_sig = image / sigma\n",
    "    img_sig_sq = image ** 2 / (1.41 * sigma ** 2)\n",
    "    img_var = image / var\n",
    "    img_var_sq = image / (var ** 2 * sigma)\n",
    "    psf_sq = psf ** 2\n",
    "\n",
    "    clean = element[\"No-noise Image\"]\n",
    "    label = tf.cast(element[\"Labels\"], tf.float32)\n",
    "\n",
    "    # Seven channel image\n",
    "    img = tf.stack([image, img_sig, img_sig_sq, img_var, img_var_sq, psf, psf_sq], axis=-1)\n",
    "\n",
    "    return ((img, stats), (clean, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scale labels\n",
    "def norm_label(label_train, label_val=None, scaler=None):\n",
    "    if not scaler:\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        label_train = scaler.fit_transform(label_train)\n",
    "        label_val = scaler.transform(label_val)\n",
    "        return label_train, label_val, scaler\n",
    "\n",
    "    else:\n",
    "        return scaler.transform(label_train)\n",
    "\n",
    "\n",
    "# Function to unscale labels\n",
    "def unnorm_label(label, scaler):\n",
    "    label = scaler.inverse_transform(label)\n",
    "\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scale images\n",
    "def norm_image(image_train, image_val=None, test=False, scaler=None):\n",
    "    if not test:\n",
    "        min_pixel = image_train.min()\n",
    "        max_pixel = image_train.max()\n",
    "        diff = max_pixel - min_pixel\n",
    "\n",
    "        image_train = ((image_train - min_pixel) / diff)[:, :, :, np.newaxis]\n",
    "        image_val = ((image_val - min_pixel) / diff)[:, :, :, np.newaxis]\n",
    "\n",
    "        return image_train, image_val, (min_pixel, diff)\n",
    "\n",
    "    else:\n",
    "        min_pixel = scaler[0]\n",
    "        diff = scaler[1]\n",
    "\n",
    "        image_train = ((image_train - min_pixel) / diff)[:, :, :, np.newaxis]\n",
    "\n",
    "        return image_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvEncoder(layers.Layer):\n",
    "    \"\"\"\n",
    "    Convolutional Encoder Layer Class.\n",
    "    Converts an input into a latent representation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_shape, dropout_rate=0.0, name=\"encoder\", **kwargs):\n",
    "        \"\"\"\n",
    "        Initializes the encoder layers and saves them as local attribute.\n",
    "        \n",
    "        Input:\n",
    "        -input_dim: 3D-tuple with (rows, cols, channels) input image dimensions.\n",
    "        \n",
    "        Returns nothing.\n",
    "        \"\"\"\n",
    "        super(ConvEncoder, self).__init__(name=name, input_shape=input_shape, **kwargs)\n",
    "\n",
    "        self.conv1 = layers.Conv2D(filters=32, kernel_size=(3, 3), padding=\"same\", strides=(2, 2))\n",
    "        self.lRelu1 = layers.LeakyReLU()\n",
    "        self.drop1 = layers.Dropout(dropout_rate)\n",
    "\n",
    "        self.conv2 = layers.Conv2D(filters=64, kernel_size=(3, 3), padding=\"same\", strides=(2, 2))\n",
    "        self.lRelu2 = layers.LeakyReLU()\n",
    "        self.drop2 = layers.Dropout(dropout_rate)\n",
    "\n",
    "        self.conv3 = layers.Conv2D(filters=128, kernel_size=(3, 3), padding=\"same\", strides=(2, 2))\n",
    "        self.lRelu3 = layers.LeakyReLU()\n",
    "        self.drop3 = layers.Dropout(dropout_rate)\n",
    "\n",
    "        self.conv4 = layers.Conv2D(filters=256, kernel_size=(3, 3), padding=\"same\", strides=(2, 2))\n",
    "        self.lRelu4 = layers.LeakyReLU()\n",
    "        self.drop4 = layers.Dropout(dropout_rate)\n",
    "\n",
    "        self.conv5 = layers.Conv2D(filters=512, kernel_size=(3, 3), padding=\"same\", strides=(2, 2))\n",
    "        self.lRelu5 = layers.LeakyReLU()\n",
    "        self.drop5 = layers.Dropout(dropout_rate)\n",
    "\n",
    "        self.conv6 = layers.Conv2D(filters=2048, kernel_size=(3, 3), padding=\"same\", strides=(2, 2))\n",
    "        self.lRelu6 = layers.LeakyReLU()\n",
    "        self.drop6 = layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        \"\"\"\n",
    "        Runs the encoding inference for `inputs`.\n",
    "        \n",
    "        Inputs:\n",
    "        -inputs: 4D-tensor with dimension (batch_size, self.input_dim).\n",
    "        \"\"\"\n",
    "\n",
    "        z = self.conv1(inputs)\n",
    "        z = self.lRelu1(z)\n",
    "        z = self.drop1(z)\n",
    "\n",
    "        z = self.conv2(z)\n",
    "        z = self.lRelu2(z)\n",
    "        z = self.drop2(z)\n",
    "\n",
    "        z = self.conv3(z)\n",
    "        z = self.lRelu3(z)\n",
    "        z = self.drop3(z)\n",
    "\n",
    "        z = self.conv4(z)\n",
    "        z = self.lRelu4(z)\n",
    "        z = self.drop4(z)\n",
    "\n",
    "        z = self.conv5(z)\n",
    "        z = self.lRelu5(z)\n",
    "        z = self.drop5(z)\n",
    "\n",
    "        z = self.conv6(z)\n",
    "        z = self.lRelu6(z)\n",
    "        z = self.drop6(z)\n",
    "\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvDecoder(layers.Layer):\n",
    "    \"\"\"\n",
    "    Convolutional Decoder Layer Class.\n",
    "    Converts z, the encoded digit vector, back into a readable digit.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_shape, dropout_rate=0.0, name=\"decoder\", **kwargs):\n",
    "        \"\"\"\n",
    "        Initializes the decoder architecture and saves it as a local attribute.\n",
    "        \n",
    "        Input:\n",
    "        -input_shape: 3D-tuple with (rows, cols, channels) input representation.\n",
    "        \n",
    "        Returns nothing.\n",
    "        \"\"\"\n",
    "        super(ConvDecoder, self).__init__(name=name, input_shape=input_shape, **kwargs)\n",
    "\n",
    "        self.convT1 = layers.Conv2DTranspose(512, kernel_size=(3, 3), strides=(2, 2), padding=\"same\")\n",
    "        self.lRelu1 = layers.LeakyReLU()\n",
    "        self.drop1 = layers.Dropout(dropout_rate)\n",
    "\n",
    "        self.convT2 = layers.Conv2DTranspose(256, kernel_size=(3, 3), strides=(2, 2), padding=\"same\")\n",
    "        self.lRelu2 = layers.LeakyReLU()\n",
    "        self.drop2 = layers.Dropout(dropout_rate)\n",
    "\n",
    "        self.convT3 = layers.Conv2DTranspose(128, kernel_size=(3, 3), strides=(2, 2), padding=\"same\")\n",
    "        self.lRelu3 = layers.LeakyReLU()\n",
    "        self.drop3 = layers.Dropout(dropout_rate)\n",
    "\n",
    "        self.convT4 = layers.Conv2DTranspose(64, kernel_size=(3, 3), strides=(2, 2), padding=\"same\")\n",
    "        self.lRelu4 = layers.LeakyReLU()\n",
    "        self.drop4 = layers.Dropout(dropout_rate)\n",
    "\n",
    "        self.convT5 = layers.Conv2DTranspose(32, kernel_size=(3, 3), strides=(2, 2), padding=\"same\")\n",
    "        self.lRelu5 = layers.LeakyReLU()\n",
    "        self.drop5 = layers.Dropout(dropout_rate)\n",
    "\n",
    "        self.convT6 = layers.Conv2DTranspose(\n",
    "            1, kernel_size=(3, 3), strides=(2, 2), padding=\"same\", activation=\"sigmoid\"\n",
    "        )\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        \"\"\"\n",
    "        Runs the encoding inference for `inputs`.\n",
    "        \n",
    "        Inputs:\n",
    "        -inputs: 4D-tensor with dimension (batch_size, self.input_dim).\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.convT1(inputs)\n",
    "        x = self.lRelu1(x)\n",
    "        x = self.drop1(x)\n",
    "\n",
    "        x = self.convT2(x)\n",
    "        x = self.lRelu2(x)\n",
    "        x = self.drop2(x)\n",
    "\n",
    "        x = self.convT3(x)\n",
    "        x = self.lRelu3(x)\n",
    "        x = self.drop3(x)\n",
    "\n",
    "        x = self.convT4(x)\n",
    "        x = self.lRelu4(x)\n",
    "        x = self.drop4(x)\n",
    "\n",
    "        x = self.convT5(x)\n",
    "        x = self.lRelu5(x)\n",
    "        x = self.drop5(x)\n",
    "\n",
    "        x = self.convT6(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Model\n",
    "def create_encoder(loss=\"binary_crossentropy\", summary=True):\n",
    "\n",
    "    # Create Encoder\n",
    "    input_encoder_img = layers.Input(input_shape_img)\n",
    "    input_encoder_stats = layers.Input(input_shape_stats)\n",
    "\n",
    "    norm_encoder_img = layers.BatchNormalization()(input_encoder_img)\n",
    "    norm_encoder_stats = layers.BatchNormalization()(input_encoder_stats)\n",
    "\n",
    "    x = ConvEncoder(input_shape_img)(norm_encoder_img)\n",
    "    x = layers.Flatten()(x)\n",
    "\n",
    "    x = layers.concatenate([x, norm_encoder_stats])\n",
    "\n",
    "    x = layers.Dense(2048)(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "\n",
    "    x = layers.Dense(1024)(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "\n",
    "    # Output later of the encoder\n",
    "    latent_z = layers.Dense(latent_z_dim_labels, activation=\"sigmoid\")(x)\n",
    "\n",
    "    # Encoder Model\n",
    "    encoder = tf.keras.Model([input_encoder_img, input_encoder_stats], latent_z, name=\"Encoder\")\n",
    "\n",
    "    # Define Optimizer\n",
    "    optimizer = optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "    # Compile the model\n",
    "    encoder.compile(optimizer, loss=loss)\n",
    "\n",
    "    # Display the model summary\n",
    "    if summary:\n",
    "        display(encoder.summary())\n",
    "\n",
    "    # Return encoder\n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Decoder Model\n",
    "def create_decoder(loss=\"binary_crossentropy\", summary=True):\n",
    "\n",
    "    # Create Decoder\n",
    "    input_decoder_labels = layers.Input(latent_z_dim_labels)\n",
    "\n",
    "    x = layers.Dense(1024)(input_decoder_labels)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "\n",
    "    x = layers.Dense(2048)(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "\n",
    "    x = layers.Reshape(latent_dim)(x)\n",
    "    recon = ConvDecoder(latent_dim)(x)\n",
    "\n",
    "    # Decoder Model\n",
    "    decoder = tf.keras.Model(input_decoder_labels, recon, name=\"Decoder\")\n",
    "\n",
    "    # Define Optimizer\n",
    "    optimizer = optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "    # Compile the model\n",
    "    decoder.compile(optimizer, loss=loss)\n",
    "\n",
    "    # Display the model summary\n",
    "    if summary:\n",
    "        display(decoder.summary())\n",
    "\n",
    "    # Return decoder\n",
    "    return decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_AE(encoder, decoder, summary=True):\n",
    "    input_encoder_img = layers.Input(input_shape_img)\n",
    "    input_encoder_stats = layers.Input(input_shape_stats)\n",
    "\n",
    "    labels = encoder([input_encoder_img, input_encoder_stats])\n",
    "    recons = decoder(labels)\n",
    "\n",
    "    # AE Model\n",
    "    AE = tf.keras.Model([input_encoder_img, input_encoder_stats], [recons, labels], name=\"AE\")\n",
    "\n",
    "    # Define Optimizer\n",
    "    optimizer = optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "    # Define Loss\n",
    "    loss = {\"Encoder\": \"binary_crossentropy\", \"Decoder\": \"binary_crossentropy\"}\n",
    "\n",
    "    # Define Loss\n",
    "    lossWeights = {\"Encoder\": 5.0, \"Decoder\": 1.0}\n",
    "\n",
    "    # Compile the model\n",
    "    AE.compile(optimizer, loss=loss, loss_weights=lossWeights)\n",
    "\n",
    "    # Display the model summary\n",
    "    if summary:\n",
    "        display(AE.summary())\n",
    "\n",
    "    # Return reverse decoder\n",
    "    return AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_or_load_AE(AE, set=1, train=False, tr_ds_AE=None, val_ds_AE=None):\n",
    "    # Scheduler for learning rate\n",
    "    def scheduler(epoch):\n",
    "        if epoch < 5:\n",
    "            return 0.001\n",
    "        else:\n",
    "            return 0.001 * np.exp(0.1 * (5 - epoch))\n",
    "\n",
    "    if not train:\n",
    "        AE.load_weights(ae_paths[set - 1])\n",
    "        AE.trainable = False\n",
    "\n",
    "    else:\n",
    "        AE.trainable = True\n",
    "        history = AE.fit(\n",
    "            tr_ds_AE,\n",
    "            epochs=epochs,\n",
    "            verbose=1,\n",
    "            validation_data=val_ds_AE,\n",
    "            callbacks=[\n",
    "                EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True),\n",
    "                LearningRateScheduler(scheduler),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        AE.save_weights(ae_paths[set - 1])\n",
    "        AE.trainable = False\n",
    "\n",
    "    return AE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
